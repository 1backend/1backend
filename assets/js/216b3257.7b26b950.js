"use strict";(self.webpackChunksingulatron_api_docs=self.webpackChunksingulatron_api_docs||[]).push([[2623],{16531:(e,s,i)=>{i.r(s),i.d(s,{assets:()=>l,contentTitle:()=>c,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"built-in-services/prompt-svc","title":"Prompt Svc","description":"The prompt service provides an easy to use interface to prompt LLMs and use AI models. Aims to serve humans and machines alike with its resilient queue based architecture.","source":"@site/docs/built-in-services/prompt-svc.md","sourceDirName":"built-in-services","slug":"/built-in-services/prompt-svc","permalink":"/docs/built-in-services/prompt-svc","draft":false,"unlisted":false,"editUrl":"https://github.com/1backend/1backend/tree/main/docs-source/docs/built-in-services/prompt-svc.md","tags":[{"inline":true,"label":"prompt-svc","permalink":"/docs/tags/prompt-svc"},{"inline":true,"label":"prompts","permalink":"/docs/tags/prompts"},{"inline":true,"label":"ai","permalink":"/docs/tags/ai"},{"inline":true,"label":"services","permalink":"/docs/tags/services"}],"version":"current","sidebarPosition":30,"frontMatter":{"sidebar_position":30,"tags":["prompt-svc","prompts","ai","services"]},"sidebar":"tutorialSidebar","previous":{"title":"Secret Svc","permalink":"/docs/built-in-services/secret-svc"},"next":{"title":"Registry Svc","permalink":"/docs/built-in-services/registry-svc"}}');var r=i(74848),n=i(28453);const o={sidebar_position:30,tags:["prompt-svc","prompts","ai","services"]},c="Prompt Svc",l={},a=[{value:"Responsibilities",id:"responsibilities",level:2},{value:"Dependencies",id:"dependencies",level:2},{value:"Current limitations",id:"current-limitations",level:2}];function d(e){const s={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,n.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.header,{children:(0,r.jsx)(s.h1,{id:"prompt-svc",children:"Prompt Svc"})}),"\n",(0,r.jsx)(s.p,{children:"The prompt service provides an easy to use interface to prompt LLMs and use AI models. Aims to serve humans and machines alike with its resilient queue based architecture."}),"\n",(0,r.jsxs)(s.blockquote,{children:["\n",(0,r.jsxs)(s.p,{children:["This page provides a high-level overview of ",(0,r.jsx)(s.code,{children:"Prompt Svc"}),". For detailed information, refer to the ",(0,r.jsx)(s.a,{href:"/docs/1backend/prompt",children:"Prompt Svc API documentation"}),"."]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"responsibilities",children:"Responsibilities"}),"\n",(0,r.jsx)(s.p,{children:"The prompt service:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Accepts prompts"}),"\n",(0,r.jsx)(s.li,{children:"Maintains a list of prompts"}),"\n",(0,r.jsx)(s.li,{children:"Processes prompts as soon as it's able to"}),"\n",(0,r.jsx)(s.li,{children:"Streams prompt answers"}),"\n",(0,r.jsx)(s.li,{children:"Handles retries of prompts that errored with an exponential backoff"}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:["It's able to stream back LLM responses, or it can respond syncronously if that's what the caller wants, for details see the ",(0,r.jsxs)(s.a,{href:"/docs/1backend/prompt",children:["Add Prompt (",(0,r.jsx)(s.code,{children:"/prompt-svc/prompt"}),") Endpoint"]}),"."]}),"\n",(0,r.jsx)(s.h2,{id:""}),"\n",(0,r.jsx)(s.h2,{id:"dependencies",children:"Dependencies"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.a,{href:"/docs/built-in-services/chat-svc",children:"Chat Svc"})," to save prompt responses to chat threads and messages"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.a,{href:"/docs/built-in-services/model-svc",children:"Model Svc"})," to get the address of the running AI models, see their status etc."]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"current-limitations",children:"Current limitations"}),"\n",(0,r.jsx)(s.p,{children:"There are planned improvements for this service:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"It should manage models: start needed ones and stop unneeded ones based on the volume of prompts in the backlog"}),"\n"]})]})}function p(e={}){const{wrapper:s}={...(0,n.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);