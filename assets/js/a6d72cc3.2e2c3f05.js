"use strict";(self.webpackChunksingulatron_api_docs=self.webpackChunksingulatron_api_docs||[]).push([[5282],{28453:(e,n,l)=>{l.d(n,{R:()=>o,x:()=>a});var s=l(96540);const t={},i=s.createContext(t);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(i.Provider,{value:n},e.children)}},94369:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"built-in-services/model-svc","title":"Model Svc","description":"The Model Svc is an AI model orchestration service that manages, deploys, and operates Large Language Models (LLMs) and other AI models across different runtime platforms with automatic GPU acceleration support.","source":"@site/docs/built-in-services/model-svc.md","sourceDirName":"built-in-services","slug":"/built-in-services/model-svc","permalink":"/docs/built-in-services/model-svc","draft":false,"unlisted":false,"editUrl":"https://github.com/1backend/1backend/tree/main/docs-source/docs/built-in-services/model-svc.md","tags":[{"inline":true,"label":"model-svc","permalink":"/docs/tags/model-svc"},{"inline":true,"label":"models","permalink":"/docs/tags/models"},{"inline":true,"label":"ai","permalink":"/docs/tags/ai"},{"inline":true,"label":"llm","permalink":"/docs/tags/llm"},{"inline":true,"label":"machine-learning","permalink":"/docs/tags/machine-learning"},{"inline":true,"label":"gpu","permalink":"/docs/tags/gpu"},{"inline":true,"label":"cuda","permalink":"/docs/tags/cuda"},{"inline":true,"label":"platforms","permalink":"/docs/tags/platforms"},{"inline":true,"label":"llamacpp","permalink":"/docs/tags/llamacpp"},{"inline":true,"label":"stable-diffusion","permalink":"/docs/tags/stable-diffusion"}],"version":"current","sidebarPosition":90,"frontMatter":{"sidebar_position":90,"tags":["model-svc","models","ai","llm","machine-learning","gpu","cuda","platforms","llamacpp","stable-diffusion"]},"sidebar":"tutorialSidebar","previous":{"title":"Deploy Svc","permalink":"/docs/built-in-services/deploy-svc"},"next":{"title":"Prompt Svc","permalink":"/docs/built-in-services/prompt-svc"}}');var t=l(74848),i=l(28453);const o={sidebar_position:90,tags:["model-svc","models","ai","llm","machine-learning","gpu","cuda","platforms","llamacpp","stable-diffusion"]},a="Model Svc",r={},d=[{value:"Architecture &amp; Purpose",id:"architecture--purpose",level:2},{value:"Key Features",id:"key-features",level:3},{value:"CLI Usage",id:"cli-usage",level:2},{value:"Starting Models",id:"starting-models",level:3},{value:"Model Status &amp; Monitoring",id:"model-status--monitoring",level:3},{value:"Model Management",id:"model-management",level:3},{value:"URL Encoding for Model IDs",id:"url-encoding-for-model-ids",level:3},{value:"Platform Architecture",id:"platform-architecture",level:2},{value:"AI Platforms",id:"ai-platforms",level:3},{value:"<strong>LlamaCpp Platform</strong>",id:"llamacpp-platform",level:4},{value:"<strong>Stable Diffusion Platform</strong>",id:"stable-diffusion-platform",level:4},{value:"GPU Acceleration (CUDA)",id:"gpu-acceleration-cuda",level:3},{value:"Model Catalog",id:"model-catalog",level:2},{value:"Popular Models",id:"popular-models",level:3},{value:"<strong>Mistral 7B</strong> (Recommended)",id:"mistral-7b-recommended",level:4},{value:"<strong>CodeLlama</strong> (Code Generation)",id:"codellama-code-generation",level:4},{value:"<strong>TinyLlama</strong> (Lightweight)",id:"tinyllama-lightweight",level:4},{value:"<strong>LLaMA2 Chat Uncensored</strong>",id:"llama2-chat-uncensored",level:4},{value:"<strong>Stable Diffusion</strong> (Image Generation)",id:"stable-diffusion-image-generation",level:4},{value:"Model Quality Levels",id:"model-quality-levels",level:3},{value:"Container Integration",id:"container-integration",level:2},{value:"Model Launch Flow",id:"model-launch-flow",level:3},{value:"Container Configuration",id:"container-configuration",level:3},{value:"Real-World Usage Examples",id:"real-world-usage-examples",level:2},{value:"1. Text Generation Setup",id:"1-text-generation-setup",level:3},{value:"2. Code Generation Workflow",id:"2-code-generation-workflow",level:3},{value:"3. Image Generation Setup",id:"3-image-generation-setup",level:3},{value:"4. Model Comparison Testing",id:"4-model-comparison-testing",level:3},{value:"5. Production Model Deployment",id:"5-production-model-deployment",level:3},{value:"6. GPU-Optimized Deployment",id:"6-gpu-optimized-deployment",level:3},{value:"Asset Management &amp; File Integration",id:"asset-management--file-integration",level:2},{value:"Model File Handling",id:"model-file-handling",level:3},{value:"Storage Requirements",id:"storage-requirements",level:3},{value:"Model Status &amp; Health Monitoring",id:"model-status--health-monitoring",level:2},{value:"Status Response Structure",id:"status-response-structure",level:3},{value:"Health Check Process",id:"health-check-process",level:3},{value:"Monitoring Commands",id:"monitoring-commands",level:3},{value:"Configuration Management",id:"configuration-management",level:2},{value:"Default Model Configuration",id:"default-model-configuration",level:3},{value:"Environment Variables",id:"environment-variables",level:3},{value:"Integration Patterns",id:"integration-patterns",level:2},{value:"With Prompt Svc",id:"with-prompt-svc",level:3},{value:"With Chat Svc",id:"with-chat-svc",level:3},{value:"With File Svc",id:"with-file-svc",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Resource Planning",id:"resource-planning",level:3},{value:"Startup Optimization",id:"startup-optimization",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"<strong>Model Won&#39;t Start</strong>",id:"model-wont-start",level:4},{value:"<strong>Out of Memory Errors</strong>",id:"out-of-memory-errors",level:4},{value:"<strong>GPU Not Detected</strong>",id:"gpu-not-detected",level:4},{value:"<strong>Model Download Failures</strong>",id:"model-download-failures",level:4},{value:"Debug Commands",id:"debug-commands",level:3},{value:"API Reference Summary",id:"api-reference-summary",level:2},{value:"Permissions &amp; Security",id:"permissions--security",level:2},{value:"Related Services",id:"related-services",level:2},{value:"Future Enhancements",id:"future-enhancements",level:2},{value:"Planned Features",id:"planned-features",level:3},{value:"Integration Roadmap",id:"integration-roadmap",level:3}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"model-svc",children:"Model Svc"})}),"\n",(0,t.jsx)(n.p,{children:"The Model Svc is an AI model orchestration service that manages, deploys, and operates Large Language Models (LLMs) and other AI models across different runtime platforms with automatic GPU acceleration support."}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["This page provides a comprehensive overview of ",(0,t.jsx)(n.code,{children:"Model Svc"}),". For detailed API information, refer to the ",(0,t.jsx)(n.a,{href:"/docs/1backend-api/start-default-model",children:"Model Svc API documentation"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"architecture--purpose",children:"Architecture & Purpose"}),"\n",(0,t.jsxs)(n.p,{children:["Model Svc serves as the ",(0,t.jsx)(n.strong,{children:"AI model management layer"})," for 1Backend, providing:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Catalog"}),": Curated collection of popular AI models (Mistral, LLaMA, CodeLlama, etc.)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Platform Abstraction"}),": Support for different AI runtimes (LlamaCpp, Stable Diffusion)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Container Orchestration"}),": Automatic deployment via ",(0,t.jsx)(n.a,{href:"/docs/built-in-services/container-svc",children:"Container Svc"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Asset Management"}),": Model file downloading and caching via ",(0,t.jsx)(n.a,{href:"/docs/built-in-services/file-svc",children:"File Svc"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPU Acceleration"}),": Automatic CUDA detection and optimization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Status Monitoring"}),": Real-time model health and readiness tracking"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"key-features",children:"Key Features"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Platform Support"}),": LlamaCpp for text generation, Stable Diffusion for image generation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Smart GPU Detection"}),": Automatic CUDA version detection and image selection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Lifecycle"}),": Start, status monitoring, default model management"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Resource Optimization"}),": Intelligent container sizing based on model requirements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Asset Caching"}),": Automatic model file downloading with resume support"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"cli-usage",children:"CLI Usage"}),"\n",(0,t.jsx)(n.p,{children:"Model Svc uses HTTP commands for all operations:"}),"\n",(0,t.jsx)(n.h3,{id:"starting-models",children:"Starting Models"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Start the default model\noo put /model-svc/default-model/start\n\n# Start a specific model by ID\noo put /model-svc/model/huggingface%2FTheBloke%2Fmistral-7b-instruct-v0.2.Q4_K_M.gguf/start\n\n# Start a lightweight model for testing\noo put /model-svc/model/huggingface%2FTheBloke%2Ftinyllama-1.1b-chat-v1.0.Q4_K_S.gguf/start\n\n# Start Stable Diffusion for image generation\noo put /model-svc/model/nicklucche%2Fstable-diffusion/start\n"})}),"\n",(0,t.jsx)(n.h3,{id:"model-status--monitoring",children:"Model Status & Monitoring"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check default model status\noo get /model-svc/default-model/status\n\n# Check specific model status\noo get /model-svc/model/huggingface%2FTheBloke%2Fmistral-7b-instruct-v0.2.Q4_K_M.gguf/status\n\n# Get model details\noo get /model-svc/model/huggingface%2FTheBloke%2Fcodellama-7b.Q4_K_M.gguf\n"})}),"\n",(0,t.jsx)(n.h3,{id:"model-management",children:"Model Management"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# List all available models\noo post /model-svc/models\n\n# List AI platforms\noo post /model-svc/platforms\n\n# Set default model\noo put /model-svc/model/huggingface%2FTheBloke%2Fmistral-7b-instruct-v0.2.Q4_K_M.gguf/make-default\n"})}),"\n",(0,t.jsx)(n.h3,{id:"url-encoding-for-model-ids",children:"URL Encoding for Model IDs"}),"\n",(0,t.jsx)(n.p,{children:"Model IDs contain special characters that must be URL-encoded:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Original ID: huggingface/TheBloke/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n# URL encoded: huggingface%2FTheBloke%2Fmistral-7b-instruct-v0.2.Q4_K_M.gguf\n\n# Use this helper for encoding\necho "huggingface/TheBloke/mistral-7b-instruct-v0.2.Q4_K_M.gguf" | jq -rR @uri\n'})}),"\n",(0,t.jsx)(n.h2,{id:"platform-architecture",children:"Platform Architecture"}),"\n",(0,t.jsx)(n.h3,{id:"ai-platforms",children:"AI Platforms"}),"\n",(0,t.jsx)(n.p,{children:"Model Svc supports multiple AI runtime platforms:"}),"\n",(0,t.jsx)(n.h4,{id:"llamacpp-platform",children:(0,t.jsx)(n.strong,{children:"LlamaCpp Platform"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n  "id": "llama-cpp",\n  "name": "Llama CPP", \n  "types": ["text-to-text"],\n  "architectures": {\n    "default": {\n      "container": {\n        "imageTemplate": "crufter/llama-cpp-python:default-1-latest",\n        "port": 8000\n      }\n    },\n    "cuda": {\n      "container": {\n        "imageTemplate": "crufter/llama-cpp-python:cuda-$cudaVersion-latest",\n        "port": 8000,\n        "envars": [{"key": "NVIDIA_VISIBLE_DEVICES", "value": "all"}]\n      },\n      "defaultCudaVersion": "12.8.0",\n      "cudaVersionPrecision": 3\n    }\n  }\n}\n'})}),"\n",(0,t.jsx)(n.h4,{id:"stable-diffusion-platform",children:(0,t.jsx)(n.strong,{children:"Stable Diffusion Platform"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n  "id": "stable-diffusion",\n  "name": "Stable Diffusion",\n  "types": ["text-to-image", "image-to-image"],\n  "architectures": {\n    "default": {\n      "container": {\n        "imageTemplate": "crufter/stable-diffusion:default-1-latest",\n        "port": 7860,\n        "envars": [{"key": "CLI_ARGS", "value": "--no-half --precision full --allow-code --enable-insecure-extension-access --api"}]\n      }\n    },\n    "cuda": {\n      "container": {\n        "imageTemplate": "crufter/stable-diffusion:cuda-$cudaVersion-latest", \n        "port": 7860\n      },\n      "defaultCudaVersion": "12.1",\n      "defaultCudnnVersion": "9"\n    }\n  }\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"gpu-acceleration-cuda",children:"GPU Acceleration (CUDA)"}),"\n",(0,t.jsx)(n.p,{children:"Model Svc automatically detects and configures GPU acceleration:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"graph TB\n    MS[Model Svc] --\x3e GD[GPU Detection]\n    GD --\x3e CV[CUDA Version Check]\n    CV --\x3e IMG{Image Available?}\n    IMG --\x3e|Yes| USE[Use GPU Image]\n    IMG --\x3e|No| FALL[Fallback to Default]\n    USE --\x3e CONT[Launch Container]\n    FALL --\x3e CONT\n    CONT --\x3e GPU[GPU-Enabled Container]\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"CUDA Version Detection:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Automatically detects system CUDA version"}),"\n",(0,t.jsx)(n.li,{children:"Matches to appropriate container image"}),"\n",(0,t.jsx)(n.li,{children:"Falls back to default version if specific image unavailable"}),"\n",(0,t.jsx)(n.li,{children:"Supports precision control (major.minor.patch)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"model-catalog",children:"Model Catalog"}),"\n",(0,t.jsx)(n.h3,{id:"popular-models",children:"Popular Models"}),"\n",(0,t.jsxs)(n.h4,{id:"mistral-7b-recommended",children:[(0,t.jsx)(n.strong,{children:"Mistral 7B"})," (Recommended)"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# High-quality general purpose model\nModel ID: huggingface/TheBloke/mistral-7b-instruct-v0.2.Q4_K_M.gguf\nSize: 4.37 GB, RAM: 6.87 GB\nQuality: Q4_K_M (medium, balanced quality - recommended)\nPrompt: [INST] {prompt} [/INST]\n"})}),"\n",(0,t.jsxs)(n.h4,{id:"codellama-code-generation",children:[(0,t.jsx)(n.strong,{children:"CodeLlama"})," (Code Generation)"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Specialized for programming tasks\nModel ID: huggingface/TheBloke/codellama-7b.Q4_K_M.gguf  \nSize: 4.24 GB, RAM: 6.74 GB\nQuality: Q4_K_M (medium, balanced quality - recommended)\nPrompt: {prompt}\n"})}),"\n",(0,t.jsxs)(n.h4,{id:"tinyllama-lightweight",children:[(0,t.jsx)(n.strong,{children:"TinyLlama"})," (Lightweight)"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Fast, minimal resource usage\nModel ID: huggingface/TheBloke/tinyllama-1.1b-chat-v1.0.Q4_K_S.gguf\nSize: 0.5 GB, RAM: 3.0 GB\nQuality: Q4_K_S (small, acceptable quality)\nPrompt: <|system|>\\n{system_message}</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\n"})}),"\n",(0,t.jsx)(n.h4,{id:"llama2-chat-uncensored",children:(0,t.jsx)(n.strong,{children:"LLaMA2 Chat Uncensored"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Uncensored conversational model\nModel ID: huggingface/TheBloke/llama2_7b_chat_uncensored.Q4_K_M.gguf\nSize: 4.08 GB, RAM: 6.58 GB\nUncensored: true\nPrompt: ### HUMAN:\\n{prompt}\\n\\n### RESPONSE:\\n\n"})}),"\n",(0,t.jsxs)(n.h4,{id:"stable-diffusion-image-generation",children:[(0,t.jsx)(n.strong,{children:"Stable Diffusion"})," (Image Generation)"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Text-to-image generation\nModel ID: nicklucche/stable-diffusion\nPlatform: stable-diffusion\nTypes: text-to-image, image-to-image\n"})}),"\n",(0,t.jsx)(n.h3,{id:"model-quality-levels",children:"Model Quality Levels"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Quality"}),(0,t.jsx)(n.th,{children:"Size"}),(0,t.jsx)(n.th,{children:"RAM Usage"}),(0,t.jsx)(n.th,{children:"Quality Loss"}),(0,t.jsx)(n.th,{children:"Recommendation"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Q2_K"}),(0,t.jsx)(n.td,{children:"Smallest"}),(0,t.jsx)(n.td,{children:"~5.5GB"}),(0,t.jsx)(n.td,{children:"Significant"}),(0,t.jsx)(n.td,{children:"Not recommended"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Q3_K_S"}),(0,t.jsx)(n.td,{children:"Very Small"}),(0,t.jsx)(n.td,{children:"~5.7GB"}),(0,t.jsx)(n.td,{children:"High"}),(0,t.jsx)(n.td,{children:"Testing only"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Q3_K_M"}),(0,t.jsx)(n.td,{children:"Small"}),(0,t.jsx)(n.td,{children:"~6.1GB"}),(0,t.jsx)(n.td,{children:"Substantial"}),(0,t.jsx)(n.td,{children:"Basic usage"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Q4_K_M"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Medium"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"~6.9GB"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Balanced"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"\u2705 Recommended"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Q5_K_M"}),(0,t.jsx)(n.td,{children:"Large"}),(0,t.jsx)(n.td,{children:"~7.3GB"}),(0,t.jsx)(n.td,{children:"Very Low"}),(0,t.jsx)(n.td,{children:"High quality"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Q6_K"}),(0,t.jsx)(n.td,{children:"Very Large"}),(0,t.jsx)(n.td,{children:"~8.0GB"}),(0,t.jsx)(n.td,{children:"Extremely Low"}),(0,t.jsx)(n.td,{children:"Maximum quality"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Q8_0"}),(0,t.jsx)(n.td,{children:"Largest"}),(0,t.jsx)(n.td,{children:"~9.7GB"}),(0,t.jsx)(n.td,{children:"Minimal"}),(0,t.jsx)(n.td,{children:"Not recommended"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"container-integration",children:"Container Integration"}),"\n",(0,t.jsx)(n.h3,{id:"model-launch-flow",children:"Model Launch Flow"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"sequenceDiagram\n    participant User\n    participant MS as Model Svc\n    participant CS as Container Svc\n    participant FS as File Svc\n    participant Docker\n    \n    User->>MS: Start Model\n    MS->>FS: Check Model Assets\n    FS--\x3e>MS: Download Status\n    alt Assets Not Ready\n        MS->>FS: Download Model Files\n        FS->>Docker: Download & Cache\n    end\n    MS->>CS: Launch Container\n    CS->>Docker: Run AI Container\n    Docker--\x3e>CS: Container Started\n    CS--\x3e>MS: Container Info\n    MS->>MS: Health Check Loop\n    MS--\x3e>User: Model Ready\n"})}),"\n",(0,t.jsx)(n.h3,{id:"container-configuration",children:"Container Configuration"}),"\n",(0,t.jsx)(n.p,{children:"Model Svc automatically configures containers based on:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Container naming (single model per node)\nContainer Name: "the-1backend-container"\nHost Port: 8001 (standardized)\n\n# GPU Configuration (when available)\nNVIDIA_VISIBLE_DEVICES=all\nGPU Capabilities: enabled\n\n# Volume Mounts (for model persistence)\nKeeps: ["/model", "/cache"]\n\n# Environment Variables (model-specific)\nMODEL=/path/to/downloaded/model.gguf\nCLI_ARGS=--specific-model-args\n'})}),"\n",(0,t.jsx)(n.h2,{id:"real-world-usage-examples",children:"Real-World Usage Examples"}),"\n",(0,t.jsx)(n.h3,{id:"1-text-generation-setup",children:"1. Text Generation Setup"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Start with a balanced general-purpose model\noo put /model-svc/model/huggingface%2FTheBloke%2Fmistral-7b-instruct-v0.2.Q4_K_M.gguf/start\n\n# Monitor startup progress\nwatch -n 2 \'oo get /model-svc/default-model/status\'\n\n# Wait for model to be ready\n# {"status": {"assetsReady": true, "running": true, "address": "localhost:8001"}}\n\n# Test with Prompt Svc\noo post /prompt-svc/prompt \\\n  --message="Explain quantum computing in simple terms" \\\n  --type="text-to-text"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-code-generation-workflow",children:"2. Code Generation Workflow"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Start CodeLlama for programming tasks\noo put /model-svc/model/huggingface%2FTheBloke%2Fcodellama-7b.Q4_K_M.gguf/start\n\n# Set as default for code tasks\noo put /model-svc/model/huggingface%2FTheBloke%2Fcodellama-7b.Q4_K_M.gguf/make-default\n\n# Generate code\noo post /prompt-svc/prompt \\\n  --message="Write a Python function to calculate Fibonacci numbers" \\\n  --type="text-to-text"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"3-image-generation-setup",children:"3. Image Generation Setup"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Start Stable Diffusion\noo put /model-svc/model/nicklucche%2Fstable-diffusion/start\n\n# Monitor status (image models take longer to start)\noo get /model-svc/model/nicklucche%2Fstable-diffusion/status\n\n# Generate images via Prompt Svc\noo post /prompt-svc/prompt \\\n  --message="A serene mountain landscape at sunset" \\\n  --type="text-to-image"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"4-model-comparison-testing",children:"4. Model Comparison Testing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Test different model sizes\nMODELS=(\n  "huggingface%2FTheBloke%2Ftinyllama-1.1b-chat-v1.0.Q4_K_S.gguf"\n  "huggingface%2FTheBloke%2Fmistral-7b-instruct-v0.2.Q3_K_M.gguf" \n  "huggingface%2FTheBloke%2Fmistral-7b-instruct-v0.2.Q4_K_M.gguf"\n)\n\nfor MODEL in "${MODELS[@]}"; do\n  echo "Testing $MODEL"\n  oo put /model-svc/model/$MODEL/start\n  sleep 30  # Wait for model to load\n  \n  # Test response quality\n  oo post /prompt-svc/prompt \\\n    --message="Write a short story about a robot learning to paint" \\\n    --type="text-to-text"\n    \n  echo "---"\ndone\n'})}),"\n",(0,t.jsx)(n.h3,{id:"5-production-model-deployment",children:"5. Production Model Deployment"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Use high-quality model for production\noo put /model-svc/model/huggingface%2FTheBloke%2Fmistral-7b-instruct-v0.2.Q5_K_M.gguf/start\n\n# Set as system default\noo put /model-svc/model/huggingface%2FTheBloke%2Fmistral-7b-instruct-v0.2.Q5_K_M.gguf/make-default\n\n# Verify configuration\noo get /model-svc/default-model/status\n\n# Test with realistic workload\noo post /prompt-svc/prompt \\\n  --message="Analyze this business proposal and provide recommendations" \\\n  --type="text-to-text"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"6-gpu-optimized-deployment",children:"6. GPU-Optimized Deployment"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check available models\noo post /model-svc/models | jq '.models[] | select(.size < 8) | {id, name, size, maxRam}'\n\n# Start model (will auto-detect GPU)\noo put /model-svc/model/huggingface%2FTheBloke%2Fmistral-7b-instruct-v0.2.Q4_K_M.gguf/start\n\n# Verify GPU usage\ndocker stats the-1backend-container\n\n# Monitor GPU memory\nnvidia-smi\n"})}),"\n",(0,t.jsx)(n.h2,{id:"asset-management--file-integration",children:"Asset Management & File Integration"}),"\n",(0,t.jsx)(n.h3,{id:"model-file-handling",children:"Model File Handling"}),"\n",(0,t.jsxs)(n.p,{children:["Model Svc integrates seamlessly with ",(0,t.jsx)(n.a,{href:"/docs/built-in-services/file-svc",children:"File Svc"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check model asset status\noo get /model-svc/model/huggingface%2FTheBloke%2Fmistral-7b-instruct-v0.2.Q4_K_M.gguf\n\n# Monitor download progress\noo get /file-svc/download/https%3A%2F%2Fhuggingface.co%2FTheBloke%2FMistral-7B-Instruct-v0.2-GGUF%2Fresolve%2Fmain%2Fmistral-7b-instruct-v0.2.Q4_K_M.gguf\n\n# List all cached models\noo post /file-svc/downloads | jq '.downloads[] | select(.url | contains(\"huggingface\"))'\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Asset Flow:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Model start requested"}),"\n",(0,t.jsx)(n.li,{children:"Check if model files exist in File Svc cache"}),"\n",(0,t.jsx)(n.li,{children:"If missing, trigger download from HuggingFace/source"}),"\n",(0,t.jsxs)(n.li,{children:["Files cached in ",(0,t.jsx)(n.code,{children:"~/.1backend/downloads/"})]}),"\n",(0,t.jsxs)(n.li,{children:["Container launched with model file mounted as ",(0,t.jsx)(n.code,{children:"$MODEL"})," environment variable"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"storage-requirements",children:"Storage Requirements"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check model storage usage\ndu -sh ~/.1backend/downloads/\n\n# Popular model sizes:\nTinyLlama 1.1B:     ~500MB\nMistral 7B Q4_K_M:  ~4.4GB\nCodeLlama 7B:       ~4.2GB\nLLaMA2 7B:          ~4.1GB\nMixtral 8x7B:       ~15-50GB (depending on quality)\nLlama 3 70B:        ~26-43GB (depending on quality)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"model-status--health-monitoring",children:"Model Status & Health Monitoring"}),"\n",(0,t.jsx)(n.h3,{id:"status-response-structure",children:"Status Response Structure"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n  "status": {\n    "assetsReady": true,\n    "running": true,\n    "address": "localhost:8001"\n  }\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Status Fields:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"assetsReady"}),": Model files downloaded and available"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"running"}),": Container is running AND responding to requests"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"address"}),": Network address where model can be accessed"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"health-check-process",children:"Health Check Process"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"graph TB\n    Start[Model Start] --\x3e Download[Download Assets]\n    Download --\x3e Launch[Launch Container]\n    Launch --\x3e Wait[Wait for Container]\n    Wait --\x3e Ping[Ping Model API]\n    Ping --\x3e Ready{Responding?}\n    Ready --\x3e|Yes| Active[Model Active]\n    Ready --\x3e|No| Retry[Retry Ping]\n    Retry --\x3e Ping\n    Active --\x3e Monitor[Continuous Monitoring]\n"})}),"\n",(0,t.jsx)(n.h3,{id:"monitoring-commands",children:"Monitoring Commands"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Continuous status monitoring\nwatch -n 5 'oo get /model-svc/default-model/status'\n\n# Check container logs if model fails\ndocker logs the-1backend-container\n\n# Monitor resource usage\ndocker stats the-1backend-container\n\n# Check model API directly\ncurl http://localhost:8001/health\n\n# List all running models\noo post /model-svc/models | jq '.models[] | {id, name, size}'\n"})}),"\n",(0,t.jsx)(n.h2,{id:"configuration-management",children:"Configuration Management"}),"\n",(0,t.jsx)(n.h3,{id:"default-model-configuration",children:"Default Model Configuration"}),"\n",(0,t.jsxs)(n.p,{children:["Model Svc stores the current default model in ",(0,t.jsx)(n.a,{href:"/docs/built-in-services/config-svc",children:"Config Svc"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check current default model\noo post /config-svc/configs \\\n  --keys='[\"modelSvc\"]' | jq '.configs.modelSvc.data.currentModelId'\n\n# The default model ID (if none set)\nDefault: \"huggingface/TheBloke/mistral-7b-instruct-v0.2.Q3_K_S.gguf\"\n"})}),"\n",(0,t.jsx)(n.h3,{id:"environment-variables",children:"Environment Variables"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# GPU platform detection\nOB_GPU_PLATFORM=cuda    # Enables CUDA support\nOB_LLM_HOST=localhost   # Override model host address\n\n# Container resource limits\nDOCKER_MEMORY_LIMIT=8g  # Container memory limit\nNVIDIA_VISIBLE_DEVICES=all  # GPU access\n"})}),"\n",(0,t.jsx)(n.h2,{id:"integration-patterns",children:"Integration Patterns"}),"\n",(0,t.jsx)(n.h3,{id:"with-prompt-svc",children:"With Prompt Svc"}),"\n",(0,t.jsxs)(n.p,{children:["Model Svc provides the AI backend for ",(0,t.jsx)(n.a,{href:"/docs/built-in-services/prompt-svc",children:"Prompt Svc"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Start model\noo put /model-svc/default-model/start\n\n# Use through Prompt Svc\noo post /prompt-svc/prompt \\\n  --message="Hello, how are you?" \\\n  --type="text-to-text"\n\n# Model address is automatically used by Prompt Svc\n# (localhost:8001)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"with-chat-svc",children:"With Chat Svc"}),"\n",(0,t.jsx)(n.p,{children:"Chat applications automatically use the default model:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Ensure model is running\noo get /model-svc/default-model/status\n\n# Create chat thread\noo post /chat-svc/thread \\\n  --threadData.title="AI Assistant"\n\n# Send message (automatically routed to Model Svc)\noo post /chat-svc/thread/thread_123/message \\\n  --text="Explain machine learning" \\\n  --userId="usr_456"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"with-file-svc",children:"With File Svc"}),"\n",(0,t.jsx)(n.p,{children:"Model assets are managed through File Svc:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Check model file cache\noo post /file-svc/downloads | grep -A 5 "mistral"\n\n# Manually download model files\noo put /file-svc/download \\\n  --url="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf" \\\n  --folderPath="/models"\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"resource-planning",children:"Resource Planning"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Small setup (< 8GB RAM)\nModel: TinyLlama 1.1B Q4_K_S\nRAM: ~3GB\nStorage: ~500MB\n\n# Medium setup (8-16GB RAM)  \nModel: Mistral 7B Q4_K_M\nRAM: ~7GB\nStorage: ~4.4GB\n\n# Large setup (16+ GB RAM)\nModel: Mistral 7B Q5_K_M  \nRAM: ~7.3GB\nStorage: ~4.8GB\n\n# GPU setup (8+ GB VRAM)\nModel: Mistral 7B Q4_K_M + CUDA\nRAM: ~7GB + GPU acceleration\nVRAM: ~4GB\n"})}),"\n",(0,t.jsx)(n.h3,{id:"startup-optimization",children:"Startup Optimization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Pre-download model files\noo put /file-svc/download \\\n  --url="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf" \\\n  --folderPath="/models"\n\n# Then start model (skips download)\noo put /model-svc/model/huggingface%2FTheBloke%2Fmistral-7b-instruct-v0.2.Q4_K_M.gguf/start\n\n# Use fastest small model for development\noo put /model-svc/model/huggingface%2FTheBloke%2Ftinyllama-1.1b-chat-v1.0.Q4_K_S.gguf/start\n'})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,t.jsx)(n.h4,{id:"model-wont-start",children:(0,t.jsx)(n.strong,{children:"Model Won't Start"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Check asset download status\noo get /model-svc/model/YOUR_MODEL_ID/status\n\n# Check file download progress  \noo post /file-svc/downloads | grep "YOUR_MODEL"\n\n# Check container logs\ndocker logs the-1backend-container\n\n# Verify model exists in catalog\noo post /model-svc/models | grep "YOUR_MODEL"\n'})}),"\n",(0,t.jsx)(n.h4,{id:"out-of-memory-errors",children:(0,t.jsx)(n.strong,{children:"Out of Memory Errors"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check available memory\nfree -h\n\n# Use smaller model\noo put /model-svc/model/huggingface%2FTheBloke%2Ftinyllama-1.1b-chat-v1.0.Q4_K_S.gguf/start\n\n# Monitor container memory\ndocker stats the-1backend-container\n"})}),"\n",(0,t.jsx)(n.h4,{id:"gpu-not-detected",children:(0,t.jsx)(n.strong,{children:"GPU Not Detected"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Check CUDA installation\nnvidia-smi\n\n# Verify Docker GPU support\ndocker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi\n\n# Check Model Svc GPU platform\necho $OB_GPU_PLATFORM  # Should be "cuda"\n'})}),"\n",(0,t.jsx)(n.h4,{id:"model-download-failures",children:(0,t.jsx)(n.strong,{children:"Model Download Failures"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Check internet connectivity\ncurl -I https://huggingface.co\n\n# Retry download manually\noo put /file-svc/download \\\n  --url="YOUR_MODEL_URL" \\\n  --folderPath="/models"\n\n# Check download errors\noo post /file-svc/downloads | jq \'.downloads[] | select(.status != "Completed")\'\n'})}),"\n",(0,t.jsx)(n.h3,{id:"debug-commands",children:"Debug Commands"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Full model information\noo get /model-svc/model/YOUR_MODEL_ID\n\n# Container status\ndocker ps | grep the-1backend-container\n\n# Model API health\ncurl http://localhost:8001/health\n\n# File system usage\ndu -sh ~/.1backend/downloads/\ndf -h\n\n# Network connectivity\ncurl -I http://localhost:8001\ntelnet localhost 8001\n"})}),"\n",(0,t.jsx)(n.h2,{id:"api-reference-summary",children:"API Reference Summary"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Endpoint"}),(0,t.jsx)(n.th,{children:"Method"}),(0,t.jsx)(n.th,{children:"Purpose"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/model-svc/default-model/start"})}),(0,t.jsx)(n.td,{children:"PUT"}),(0,t.jsx)(n.td,{children:"Start the default model"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/model-svc/model/{modelId}/start"})}),(0,t.jsx)(n.td,{children:"PUT"}),(0,t.jsx)(n.td,{children:"Start specific model"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/model-svc/default-model/status"})}),(0,t.jsx)(n.td,{children:"GET"}),(0,t.jsx)(n.td,{children:"Get default model status"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/model-svc/model/{modelId}/status"})}),(0,t.jsx)(n.td,{children:"GET"}),(0,t.jsx)(n.td,{children:"Get specific model status"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/model-svc/models"})}),(0,t.jsx)(n.td,{children:"POST"}),(0,t.jsx)(n.td,{children:"List all available models"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/model-svc/platforms"})}),(0,t.jsx)(n.td,{children:"POST"}),(0,t.jsx)(n.td,{children:"List AI platforms"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/model-svc/model/{modelId}"})}),(0,t.jsx)(n.td,{children:"GET"}),(0,t.jsx)(n.td,{children:"Get model details"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"/model-svc/model/{modelId}/make-default"})}),(0,t.jsx)(n.td,{children:"PUT"}),(0,t.jsx)(n.td,{children:"Set model as default"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"permissions--security",children:"Permissions & Security"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Required permissions\nmodel-svc:model:create     # Start models\nmodel-svc:model:view       # View model status and list\nmodel-svc:platform:view    # List platforms\n\n# Service permissions (automatic)\nprompt-svc -> model-svc:model:view        # Prompt Svc can query models\nprompt-svc -> model-svc:platform:view     # Prompt Svc can list platforms\n"})}),"\n",(0,t.jsx)(n.h2,{id:"related-services",children:"Related Services"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/docs/built-in-services/container-svc",children:"Container Svc"})}),": Runs AI model containers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/docs/built-in-services/file-svc",children:"File Svc"})}),": Downloads and caches model files"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/docs/built-in-services/prompt-svc",children:"Prompt Svc"})}),": Sends prompts to running models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/docs/built-in-services/chat-svc",children:"Chat Svc"})}),": AI-powered chat applications"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/docs/built-in-services/config-svc",children:"Config Svc"})}),": Stores default model configuration"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"future-enhancements",children:"Future Enhancements"}),"\n",(0,t.jsx)(n.h3,{id:"planned-features",children:"Planned Features"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Model Support"}),": Run multiple models simultaneously"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Quantization"}),": Dynamic quality adjustment based on available resources"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Auto-Scaling"}),": Scale model instances based on request load"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Marketplace"}),": Extended catalog with community models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fine-Tuning"}),": Support for custom model training and fine-tuning"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"integration-roadmap",children:"Integration Roadmap"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cloud Providers"}),": Integration with cloud GPU services (AWS, GCP, Azure)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Serving"}),": Dedicated model serving platforms (TensorRT, TorchServe)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Distributed Inference"}),": Model sharding across multiple nodes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"A/B Testing"}),": Compare different models for the same task"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cost Optimization"}),": Automatic model selection based on cost/performance"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Model Svc provides the foundation for AI-powered applications in 1Backend, handling everything from model deployment to health monitoring with automatic GPU acceleration and intelligent resource management."})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);