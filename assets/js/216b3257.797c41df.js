"use strict";(self.webpackChunksingulatron_api_docs=self.webpackChunksingulatron_api_docs||[]).push([[2623],{16531:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>a,contentTitle:()=>c,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"built-in-services/prompt-svc","title":"Prompt Svc","description":"The prompt service provides an easy to use interface to prompt LLMs and use AI models. Aims to serve humans and machines alike with its resilient queue based architecture.","source":"@site/docs/built-in-services/prompt-svc.md","sourceDirName":"built-in-services","slug":"/built-in-services/prompt-svc","permalink":"/docs/built-in-services/prompt-svc","draft":false,"unlisted":false,"editUrl":"https://github.com/1backend/1backend/tree/main/docs-source/docs/built-in-services/prompt-svc.md","tags":[{"inline":true,"label":"prompt-svc","permalink":"/docs/tags/prompt-svc"},{"inline":true,"label":"prompts","permalink":"/docs/tags/prompts"},{"inline":true,"label":"ai","permalink":"/docs/tags/ai"},{"inline":true,"label":"services","permalink":"/docs/tags/services"}],"version":"current","sidebarPosition":30,"frontMatter":{"sidebar_position":30,"tags":["prompt-svc","prompts","ai","services"]},"sidebar":"tutorialSidebar","previous":{"title":"Secret Svc","permalink":"/docs/built-in-services/secret-svc"},"next":{"title":"Registry Svc","permalink":"/docs/built-in-services/registry-svc"}}');var n=t(74848),r=t(28453);const o={sidebar_position:30,tags:["prompt-svc","prompts","ai","services"]},c="Prompt Svc",a={},l=[{value:"Responsibilities",id:"responsibilities",level:2},{value:"Dependencies",id:"dependencies",level:2},{value:"Current limitations",id:"current-limitations",level:2}];function d(e){const s={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(s.header,{children:(0,n.jsx)(s.h1,{id:"prompt-svc",children:"Prompt Svc"})}),"\n",(0,n.jsx)(s.p,{children:"The prompt service provides an easy to use interface to prompt LLMs and use AI models. Aims to serve humans and machines alike with its resilient queue based architecture."}),"\n",(0,n.jsxs)(s.blockquote,{children:["\n",(0,n.jsxs)(s.p,{children:["This page provides a high-level overview of ",(0,n.jsx)(s.code,{children:"Prompt Svc"}),". For detailed information, refer to the ",(0,n.jsx)(s.a,{href:"/docs/1backend/prompt",children:"Prompt Svc API documentation"}),"."]}),"\n"]}),"\n",(0,n.jsx)(s.h2,{id:"responsibilities",children:"Responsibilities"}),"\n",(0,n.jsx)(s.p,{children:"The prompt service:"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"Accepts prompts"}),"\n",(0,n.jsx)(s.li,{children:"Maintains a list of prompts"}),"\n",(0,n.jsx)(s.li,{children:"Processes prompts as soon as it's able to"}),"\n",(0,n.jsx)(s.li,{children:"Streams prompt answers"}),"\n",(0,n.jsx)(s.li,{children:"Handles retries of prompts that errored with an exponential backoff"}),"\n"]}),"\n",(0,n.jsxs)(s.p,{children:["It's able to stream back LLM responses, or it can respond syncronously if that's what the caller wants, for details see the ",(0,n.jsxs)(s.a,{href:"/docs/1backend/prompt",children:["Add Prompt (",(0,n.jsx)(s.code,{children:"/prompt-svc/prompt"}),") Endpoint"]}),"."]}),"\n",(0,n.jsx)(s.h2,{id:""}),"\n",(0,n.jsx)(s.h2,{id:"dependencies",children:"Dependencies"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.a,{href:"/docs/built-in-services/chat-svc",children:"Chat Svc"})," to save prompt responses to chat threads and messages"]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.a,{href:"/docs/built-in-services/model-svc",children:"Model Svc"})," to get the address of the running AI models, see their status etc."]}),"\n"]}),"\n",(0,n.jsx)(s.h2,{id:"current-limitations",children:"Current limitations"}),"\n",(0,n.jsx)(s.p,{children:"There are planned improvements for this service:"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"It should manage models: start needed ones and stop unneeded ones based on the volume of prompts in the backlog"}),"\n"]})]})}function p(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,n.jsx)(s,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},28453:(e,s,t)=>{t.d(s,{R:()=>o,x:()=>c});var i=t(96540);const n={},r=i.createContext(n);function o(e){const s=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function c(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),i.createElement(r.Provider,{value:s},e.children)}}}]);